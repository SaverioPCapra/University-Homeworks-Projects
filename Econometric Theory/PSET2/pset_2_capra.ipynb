{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSET 2 - Econometric Theory - Saverio Pietro Capra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I import the Python modules (or libraries) that I'll need to solve the problems\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I read the csv of the dataset and I assign it to a variable\n",
    "data = pd.read_csv(\"tracking.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "Here below you can see some functions that I've defined in order to make the code below look neater."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{ATE} = \\frac{1}{n_{1}} \\sum^{n}_{i=1} D_{i}y_{i}(1)- \\frac{1}{n-n_{1}} \\sum^{n}_{i=1} (1-D_{i})y_{i}(0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I define a function to compute the difference between the mean of the treatment and non-treatment effect\n",
    "\n",
    "def ATE_estimator(df, col_name:str, variable_name:str):\n",
    "    \"\"\"\n",
    "    df -> dataframe which contains the data on which we want to estimate the ATE\n",
    "    col -> the column which indicates whether the unit received a treatment or not (1 if treatment, 0 if no treatment)\n",
    "    variable -> the variable with respect to which you want to compute the ATE, so the variable of interest\n",
    "    \"\"\"\n",
    "    # I calculate the mean of the treatment group\n",
    "    treatment_mean = df[df[col_name]== 1][variable_name].mean()\n",
    "\n",
    "    # I calculate the mean of the non-treatment group\n",
    "    non_treatment_mean = df[df[col_name]== 0][variable_name].mean()\n",
    "\n",
    "    # I calculate the difference between the two means, to estimate the ATE\n",
    "    ATE = treatment_mean - non_treatment_mean\n",
    "\n",
    "    # I return the ATE\n",
    "    return ATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the effect of tracking on students' end of first grade test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I group the data by schoolid and remove the column which says whether students were above or below the median, since I don't need it right now\n",
    "# I also calculate the mean scoreendfirstgrade for each school\n",
    "grouped_data = data[[\"tracking\", \"scoreendfirstgrade\", \"schoolid\"]].groupby([\"schoolid\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tracking</th>\n",
       "      <th>scoreendfirstgrade</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>schoolid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.184369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.178371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.068224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.024504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.757769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tracking  scoreendfirstgrade\n",
       "schoolid                              \n",
       "430            1.0           -0.184369\n",
       "432            1.0           -0.178371\n",
       "436            1.0           -0.068224\n",
       "443            0.0           -0.024504\n",
       "451            0.0           -0.757769"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here I show a snapshot of the new dataframe I created\n",
    "grouped_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13391256466638107"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ordinary calculation of the ATE estimator, as shown in the lecture notes, approach\n",
    "\n",
    "ATE_estimator(grouped_data, \"tracking\", \"scoreendfirstgrade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13391256466638107"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regression approach\n",
    "ATE_estimation = sm.OLS(grouped_data[\"scoreendfirstgrade\"], sm.add_constant(grouped_data[\"tracking\"])).fit().params[1]\n",
    "ATE_estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this simple estimate it seems that tracking has a positive effect on end of first grade scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a 10% level randomization inference test to assess whether the finding of question 1\n",
    "is robust or whether it rests on an unappropriate asymptotic approximation. Conclude based\n",
    "on the result of this randomization test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total numbe of schools involved in the study is 108\n",
      "Among these schools 60 were assigned the treatment, and 48 were not assigned the treatment\n"
     ]
    }
   ],
   "source": [
    "print(\"The total numbe of schools involved in the study is\",grouped_data.shape[0])\n",
    "\n",
    "treatment_schools = round(grouped_data[grouped_data[\"tracking\"]==1].sum()[0])\n",
    "no_treatment_schools = round(grouped_data[grouped_data[\"tracking\"]==0].count()[0])\n",
    "print(f\"Among these schools {treatment_schools} were assigned the treatment, and {no_treatment_schools} were not assigned the treatment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the 10% randomization inference test, I referenced page 151 of \"Slides_RCT.pdf\" where the steps to carry out a randomization inference are indicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's a randomization inference, I work under the following assumption (sharp null hypothesis): $$H^{s}_{0}:y_{i}(1)=y_{i}(0)\\; \\forall i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I define the level of the confidence interval\n",
    "alpha = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I randomly draw a subset of 10000 possible values for $(D_{1},\\dots, D_{n})$.\n",
    "\n",
    "I estimate $\\hat{ATE}_{n}$ for these values (under the sharp null).\n",
    "\n",
    "Reference: Slide 151 - Slides_RCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of permutations that I will run to carry out the randomization inference\n",
    "num_permutations = 1000\n",
    "\n",
    "# Here I store the outcome of each simulation\n",
    "permutation_ATEs = []\n",
    "\n",
    "# This is a for-loop that repeats for 10000 times\n",
    "for i in range(num_permutations):\n",
    "    # Creates a permuted version of the DataFrame by randomly shuffling the rows without replacement\n",
    "    permuted_df = grouped_data.sample(frac=1, replace=False)\n",
    "\n",
    "    # I assign to this variable the number of units to which I will have to assign the treatment in the MC simulation \n",
    "    # n_treatment = number of schools that received treatment, which is 60\n",
    "    # Note: I tried by assigning the treatment to 50% of the schools and the result is the same\n",
    "    n_treatment = treatment_schools\n",
    "\n",
    "    # I assign to the first 60 rows (number of schools that receive treatment) of the permuted dataframe the treatment\n",
    "    treatment_df = permuted_df.copy().iloc[:n_treatment]\n",
    "    treatment_df[\"tracking\"] =  1\n",
    "\n",
    "    # I leave the remaining entries untreated\n",
    "    no_treatment_df = permuted_df.copy().iloc[n_treatment:]\n",
    "    no_treatment_df[\"tracking\"] = 0\n",
    "\n",
    "    # I concatenate the treatment and non-treatment dataframe\n",
    "    # I do this, so that then I can run my function to estimate the ATE\n",
    "    complete_df = pd.concat([treatment_df, no_treatment_df], axis = 0)\n",
    "\n",
    "    # I estimate the ATE by using the function I defined above\n",
    "    permutation_ATE = ATE_estimator(complete_df, \"tracking\", \"scoreendfirstgrade\")\n",
    "\n",
    "    # I append to the list permutation_t_stats my ATE estimate\n",
    "    permutation_ATEs.append(permutation_ATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the $(1 - \\frac{\\alpha}{2})$ th quantile and $\\frac{\\alpha}{2}$ th quantile of the 10000 values of $\\hat{ATE}_{n}$ that I've estimated.\n",
    "\n",
    "In this way I \"estimate\" the true $(1 - \\frac{\\alpha}{2})$ th quantile and $\\frac{\\alpha}{2}$ th quantile of $\\hat{ATE}_{n}$  across its ${n \\choose n_{1}}$ possible values\n",
    "\n",
    "Reference: Slide 151 - Slides_RCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical values: (-0.13527290706394515, 0.1331496285509487)\n"
     ]
    }
   ],
   "source": [
    "# Determine critical value, which is the 10 percentile value of the permutation_t_stats list, which has all the t_stats of the MC simulations\n",
    "critical_value1 = np.percentile(permutation_ATEs, alpha/2)\n",
    "critical_value2 = np.percentile(permutation_ATEs, 100-alpha/2)\n",
    "\n",
    "print(\"Critical values:\", (critical_value1, critical_value2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I reject the sharp null if the value taken by $\\hat{ATE}_{n}$ in the original experiment is outside the range of values defined by the two quantiles computed in the previous step.\n",
    "\n",
    "Reference: Slide 151 - Slides_RCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding is robust at 10% level randomization inference\n"
     ]
    }
   ],
   "source": [
    "# Compare observed statistic to critical value\n",
    "\n",
    "# If my estimate for the ATE is outside the range of values defined by the two quantiles, I reject\n",
    "# the null\n",
    "if ATE_estimation < critical_value1 or ATE_estimation > critical_value2:\n",
    "    print(\"Finding is robust at 10% level randomization inference\")\n",
    "else:\n",
    "    print(\"Finding is not robust at 10% level randomization inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finding seems robust, so we can reject the null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might fear that tracking benefits strong students while harming weaker ones. Assess\n",
    "whether this is a legitimate concern using the dataset at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference in means between treated and non-treated for students BELOW the median is 0.13009291684380003\n",
      "The difference in means between treated and non-treated for students ABOVE the median is 0.14925006025040954\n"
     ]
    }
   ],
   "source": [
    "# First of all I try to compute a simple difference in means\n",
    "\n",
    "# I isolate only the students below the median and I compute the mean difference between treatment and non-treatment group\n",
    "ATE_bottom_half = ATE_estimator(data[data[\"bottomhalf\"]==1], \"tracking\", \"scoreendfirstgrade\")\n",
    "print(\"The difference in means between treated and non-treated for students BELOW the median is\", ATE_bottom_half)\n",
    "\n",
    "# I isolate only the students above the median and I compute the mean difference between treatment and non-treatment group\n",
    "ATE_upper_half = ATE_estimator(data[data[\"bottomhalf\"]==0], \"tracking\", \"scoreendfirstgrade\")\n",
    "print(\"The difference in means between treated and non-treated for students ABOVE the median is\", ATE_upper_half)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average treatment effect seems to be positive in both cases (students below and above the mean), and with similar values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I carry out an analogous estimate with a regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each school I compute the mean scoreendfirstgrade for students below and above the median (as you can see in the dataframe below)\n",
    "grouped_schools_wrtmedian = data.groupby([\"schoolid\",\"bottomhalf\"]).mean().reset_index()\n",
    "grouped_schools_wrtmedian = grouped_schools_wrtmedian.set_index(\"schoolid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bottomhalf</th>\n",
       "      <th>tracking</th>\n",
       "      <th>scoreendfirstgrade</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>schoolid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.050195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.410246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.180203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.509363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.610353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          bottomhalf  tracking  scoreendfirstgrade\n",
       "schoolid                                          \n",
       "430                0       1.0            0.050195\n",
       "430                1       1.0           -0.410246\n",
       "432                0       1.0            0.180203\n",
       "432                1       1.0           -0.509363\n",
       "436                0       1.0            0.610353"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_schools_wrtmedian.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I create two separate dataframes, in the first one I isolate the average bottom half performance for each school, while in the second I isolate the average result of above median students \n",
    "grouped_schools_bottomhalf = grouped_schools_wrtmedian[grouped_schools_wrtmedian[\"bottomhalf\"]==1]\n",
    "grouped_schools_upperhalf = grouped_schools_wrtmedian[grouped_schools_wrtmedian[\"bottomhalf\"]==0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bottomhalf</th>\n",
       "      <th>tracking</th>\n",
       "      <th>scoreendfirstgrade</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>schoolid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.410246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.509363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.777645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.499816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.989415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          bottomhalf  tracking  scoreendfirstgrade\n",
       "schoolid                                          \n",
       "430                1       1.0           -0.410246\n",
       "432                1       1.0           -0.509363\n",
       "436                1       1.0           -0.777645\n",
       "443                1       0.0           -0.499816\n",
       "451                1       0.0           -0.989415"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_schools_bottomhalf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bottomhalf</th>\n",
       "      <th>tracking</th>\n",
       "      <th>scoreendfirstgrade</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>schoolid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.050195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.180203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.610353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.450807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.633036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          bottomhalf  tracking  scoreendfirstgrade\n",
       "schoolid                                          \n",
       "430                0       1.0            0.050195\n",
       "432                0       1.0            0.180203\n",
       "436                0       1.0            0.610353\n",
       "443                0       0.0            0.450807\n",
       "451                0       0.0           -0.633036"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_schools_upperhalf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I regress the mean performance of originally below median students for each school over the dummy which indicates whether students at that school were tracked or not\n",
    "# Note: in the regression I add a constant\n",
    "\n",
    "model_bottomhalf = sm.OLS(grouped_schools_bottomhalf[\"scoreendfirstgrade\"], sm.add_constant(grouped_schools_bottomhalf[\"tracking\"])).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>scoreendfirstgrade</td> <th>  R-squared:         </th> <td>   0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                    <td>OLS</td>        <th>  Adj. R-squared:    </th> <td>   0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>              <td>Least Squares</td>   <th>  F-statistic:       </th> <td>   3.429</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>              <td>Wed, 16 Oct 2024</td>  <th>  Prob (F-statistic):</th>  <td>0.0668</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                  <td>18:46:58</td>      <th>  Log-Likelihood:    </th> <td> -53.205</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>       <td>   108</td>       <th>  AIC:               </th> <td>   110.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>           <td>   106</td>       <th>  BIC:               </th> <td>   115.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>               <td>     1</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>       <td>nonrobust</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>   -0.4793</td> <td>    0.058</td> <td>   -8.308</td> <td> 0.000</td> <td>   -0.594</td> <td>   -0.365</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tracking</th> <td>    0.1433</td> <td>    0.077</td> <td>    1.852</td> <td> 0.067</td> <td>   -0.010</td> <td>    0.297</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 7.707</td> <th>  Durbin-Watson:     </th> <td>   1.574</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.021</td> <th>  Jarque-Bera (JB):  </th> <td>   7.717</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.653</td> <th>  Prob(JB):          </th> <td>  0.0211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.097</td> <th>  Cond. No.          </th> <td>    2.77</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    & scoreendfirstgrade & \\textbf{  R-squared:         } &     0.031   \\\\\n",
       "\\textbf{Model:}            &        OLS         & \\textbf{  Adj. R-squared:    } &     0.022   \\\\\n",
       "\\textbf{Method:}           &   Least Squares    & \\textbf{  F-statistic:       } &     3.429   \\\\\n",
       "\\textbf{Date:}             &  Wed, 16 Oct 2024  & \\textbf{  Prob (F-statistic):} &   0.0668    \\\\\n",
       "\\textbf{Time:}             &      18:46:58      & \\textbf{  Log-Likelihood:    } &   -53.205   \\\\\n",
       "\\textbf{No. Observations:} &          108       & \\textbf{  AIC:               } &     110.4   \\\\\n",
       "\\textbf{Df Residuals:}     &          106       & \\textbf{  BIC:               } &     115.8   \\\\\n",
       "\\textbf{Df Model:}         &            1       & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &     nonrobust      & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                  & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}    &      -0.4793  &        0.058     &    -8.308  &         0.000        &       -0.594    &       -0.365     \\\\\n",
       "\\textbf{tracking} &       0.1433  &        0.077     &     1.852  &         0.067        &       -0.010    &        0.297     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  7.707 & \\textbf{  Durbin-Watson:     } &    1.574  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.021 & \\textbf{  Jarque-Bera (JB):  } &    7.717  \\\\\n",
       "\\textbf{Skew:}          &  0.653 & \\textbf{  Prob(JB):          } &   0.0211  \\\\\n",
       "\\textbf{Kurtosis:}      &  3.097 & \\textbf{  Cond. No.          } &     2.77  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:     scoreendfirstgrade   R-squared:                       0.031\n",
       "Model:                            OLS   Adj. R-squared:                  0.022\n",
       "Method:                 Least Squares   F-statistic:                     3.429\n",
       "Date:                Wed, 16 Oct 2024   Prob (F-statistic):             0.0668\n",
       "Time:                        18:46:58   Log-Likelihood:                -53.205\n",
       "No. Observations:                 108   AIC:                             110.4\n",
       "Df Residuals:                     106   BIC:                             115.8\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -0.4793      0.058     -8.308      0.000      -0.594      -0.365\n",
       "tracking       0.1433      0.077      1.852      0.067      -0.010       0.297\n",
       "==============================================================================\n",
       "Omnibus:                        7.707   Durbin-Watson:                   1.574\n",
       "Prob(Omnibus):                  0.021   Jarque-Bera (JB):                7.717\n",
       "Skew:                           0.653   Prob(JB):                       0.0211\n",
       "Kurtosis:                       3.097   Cond. No.                         2.77\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the results of the regression\n",
    "# Tracking had a positive effect on the performance of below median students \n",
    "# This result is statistically significant with respect to an alpha of 10%\n",
    "\n",
    "model_bottomhalf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I regress the mean performance of originally above median students for each school over the dummy which indicates whether students at that school were tracked or not\n",
    "# Note: in the regression I add a constant\n",
    "\n",
    "model_upperhalf = sm.OLS(grouped_schools_upperhalf[\"scoreendfirstgrade\"], sm.add_constant(grouped_schools_upperhalf[\"tracking\"])).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>scoreendfirstgrade</td> <th>  R-squared:         </th> <td>   0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                    <td>OLS</td>        <th>  Adj. R-squared:    </th> <td>   0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>              <td>Least Squares</td>   <th>  F-statistic:       </th> <td>   2.518</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>              <td>Wed, 16 Oct 2024</td>  <th>  Prob (F-statistic):</th>  <td> 0.116</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                  <td>18:47:04</td>      <th>  Log-Likelihood:    </th> <td> -78.779</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>       <td>   108</td>       <th>  AIC:               </th> <td>   161.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>           <td>   106</td>       <th>  BIC:               </th> <td>   166.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>               <td>     1</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>       <td>nonrobust</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>    0.3063</td> <td>    0.073</td> <td>    4.189</td> <td> 0.000</td> <td>    0.161</td> <td>    0.451</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tracking</th> <td>    0.1556</td> <td>    0.098</td> <td>    1.587</td> <td> 0.116</td> <td>   -0.039</td> <td>    0.350</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.585</td> <th>  Durbin-Watson:     </th> <td>   1.530</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.453</td> <th>  Jarque-Bera (JB):  </th> <td>   1.508</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.184</td> <th>  Prob(JB):          </th> <td>   0.470</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.553</td> <th>  Cond. No.          </th> <td>    2.77</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    & scoreendfirstgrade & \\textbf{  R-squared:         } &     0.023   \\\\\n",
       "\\textbf{Model:}            &        OLS         & \\textbf{  Adj. R-squared:    } &     0.014   \\\\\n",
       "\\textbf{Method:}           &   Least Squares    & \\textbf{  F-statistic:       } &     2.518   \\\\\n",
       "\\textbf{Date:}             &  Wed, 16 Oct 2024  & \\textbf{  Prob (F-statistic):} &    0.116    \\\\\n",
       "\\textbf{Time:}             &      18:47:04      & \\textbf{  Log-Likelihood:    } &   -78.779   \\\\\n",
       "\\textbf{No. Observations:} &          108       & \\textbf{  AIC:               } &     161.6   \\\\\n",
       "\\textbf{Df Residuals:}     &          106       & \\textbf{  BIC:               } &     166.9   \\\\\n",
       "\\textbf{Df Model:}         &            1       & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &     nonrobust      & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                  & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}    &       0.3063  &        0.073     &     4.189  &         0.000        &        0.161    &        0.451     \\\\\n",
       "\\textbf{tracking} &       0.1556  &        0.098     &     1.587  &         0.116        &       -0.039    &        0.350     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  1.585 & \\textbf{  Durbin-Watson:     } &    1.530  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.453 & \\textbf{  Jarque-Bera (JB):  } &    1.508  \\\\\n",
       "\\textbf{Skew:}          &  0.184 & \\textbf{  Prob(JB):          } &    0.470  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.553 & \\textbf{  Cond. No.          } &     2.77  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:     scoreendfirstgrade   R-squared:                       0.023\n",
       "Model:                            OLS   Adj. R-squared:                  0.014\n",
       "Method:                 Least Squares   F-statistic:                     2.518\n",
       "Date:                Wed, 16 Oct 2024   Prob (F-statistic):              0.116\n",
       "Time:                        18:47:04   Log-Likelihood:                -78.779\n",
       "No. Observations:                 108   AIC:                             161.6\n",
       "Df Residuals:                     106   BIC:                             166.9\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.3063      0.073      4.189      0.000       0.161       0.451\n",
       "tracking       0.1556      0.098      1.587      0.116      -0.039       0.350\n",
       "==============================================================================\n",
       "Omnibus:                        1.585   Durbin-Watson:                   1.530\n",
       "Prob(Omnibus):                  0.453   Jarque-Bera (JB):                1.508\n",
       "Skew:                           0.184   Prob(JB):                        0.470\n",
       "Kurtosis:                       2.553   Cond. No.                         2.77\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the results of the regression\n",
    "# Tracking had a positive effect on the performance of above median students \n",
    "# This result is statistically significant with respect to an alpha of 15%\n",
    "\n",
    "model_upperhalf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases (for students originally below or above the median), it seems that tracking has a positive effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove the following theorem:\n",
    "\n",
    "Theorem 0.0.1 Using draws from a uniform distribution to generate draws from other continuous distributions.\n",
    "\n",
    "Let $F$ denote a strictly increasing cdf. If $U$ follows the uniform [0, 1] distribution, then the cdf of $F^{-1}(U)$ is $F$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof\n",
    "\n",
    "$F$ is a strictly increasing cdf, and therefore it has an inverse, which is $F^{-1}$.\n",
    "\n",
    "$F$ takes as input a value $x$ and returns the probability that the random variable $X$ assumes a value that is lower than $x$.\n",
    "\n",
    "$F^{-1}$ works in the opposite way: it receives as input a number $u$ between 0 and 1, which is a probability, and returns the value $x$ such that $P(X \\leq x)= u$.\n",
    "\n",
    "We know that the CDF of $F^{-1}(U)$ can be written in the following way: \n",
    "\n",
    "$$G(x)=P(F^{-1}(U) \\leq x)$$\n",
    "\n",
    "I rewrite both terms inside the function as a function of $F$: \n",
    "\n",
    "$$G(x)=P(F(F^{-1}(U)) \\leq F(x))$$\n",
    "$$G(x)=P(U \\leq F(x))$$\n",
    "\n",
    "Since $U$ is uniformly distributed, then: \n",
    "$$G(x)=P(U \\leq F(x)) = F(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I define the variable obs which contains the number of observations\n",
    "obs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I define the variable V which has 1000 values taken randomly from a standard normal distribution\n",
    "V = np.random.normal(loc=0, scale=1, size=obs)\n",
    "\n",
    "# I define the variable y_0 which has 1000 values taken randomly from a standard normal distribution\n",
    "y_0 = np.random.normal(loc=0, scale=1, size=obs)\n",
    "\n",
    "# I define the variable y_1 as indicated in the instructions\n",
    "y_1 = 0.5*y_0+0.5*V+0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the way we created `y_0` and `y_1` the treatment effect is heterogeneous across units, since `y_1 - y_0 = 0.5(V-y_0)+0.2`, where `V` and `y_0` are different for each unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE_1000: 0.20296737587317484\n"
     ]
    }
   ],
   "source": [
    "# I simply compute the mean of the differences between y_1 and y_0\n",
    "ATE_1000 = np.mean(y_1-y_0)\n",
    "\n",
    "print(\"ATE_1000:\", ATE_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation coefficient is 0.7086312792913708\n",
      "The variance of (y_1-y_0) is 0.5134004722492221\n"
     ]
    }
   ],
   "source": [
    "# Correlation between y_0 and y_1\n",
    "corr_y0y1 = np.corrcoef(y_1, y_0)\n",
    "print(\"The correlation coefficient is\", corr_y0y1[0][1])\n",
    "\n",
    "# Variance between y_0 and y_1\n",
    "var_y0y1 = np.var(y_1-y_0)\n",
    "print(\"The variance of (y_1-y_0) is\", var_y0y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I assign to the variable iterations the number of times the loop will have to repeat itself\n",
    "iterations = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list below is where I will store the ATE estimates for each iteration\n",
    "ATE_estimator_list0 = []\n",
    "# The list below is where I will store the upper bound ATE variance estimates for each iteration\n",
    "Upper_bound_ATE_variance0 = []\n",
    "# The list below is where I will the outcome of the indicator function wrt to the confidence interval\n",
    "# As indicated in the instructions\n",
    "# The indicator that is 1 if the actual ATE is in the confidence interval and 0 otherwise\n",
    "Indicator_ATEparam_inCI0 = []\n",
    "\n",
    "# The list below is where I will store the ATE estimates for each iteration (with manual computation, non reg-coefficient)\n",
    "ATE_estimator_manual0 = []\n",
    "# The list below is where I will store the upper bound ATE variance estimates for each iteration, compute manually, non with heteroscedasticity\n",
    "# robust standard error\n",
    "Upper_bound_ATE_manual0 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I define the for loop (800 iterations, as stated above)\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    # I create a dataframe which stores for all units the potential outcomes, which I defined in part 2 of exercise 2\n",
    "    units = pd.DataFrame({\"Y0\": y_0, \"Y1\": y_1})\n",
    "\n",
    "    # I create a variable containing a vector of random numbers (1000, in total) taken from a uniform distribution\n",
    "    r_num = np.random.uniform(0, 1, 1000)\n",
    "\n",
    "    # I add to the dataframe units a column with the random numbers, and then I sort the dataframe\n",
    "    # based on this new column of random numbers\n",
    "    units.insert(2, \"Random Number\", r_num, False)\n",
    "    units = units.sort_values(by=\"Random Number\")\n",
    "\n",
    "    # I create a dummy variable D equal to 1 for the first 500 observations in the sorted dataset\n",
    "    D = [1]*500+[0]*500\n",
    "    # I add this dummy variable as the 4th column of the dataframe\n",
    "    units.insert(3, \"Dummy\", D)\n",
    "\n",
    "    # I create a variable Y = (1-D)y(0)+Dy(1)\n",
    "    Y = (np.ones(1000)-units[\"Dummy\"].values)*units[\"Y0\"].values+units[\"Dummy\"].values*units[\"Y1\"].values\n",
    "\n",
    "    # I regress Y on D using the robust option\n",
    "    rlr_model = sm.RLM(Y,sm.add_constant(units[\"Dummy\"].values), M=sm.robust.norms.HuberT())\n",
    "    # I extract the coefficient of the dummy variable, since it should correspond to the estimate of ATE\n",
    "    ATE_estimate_D = rlr_model.fit().params[1]\n",
    "    # I append to a Python list the number ATE_estimate_D\n",
    "    ATE_estimator_list0.append(ATE_estimate_D)\n",
    "\n",
    "    # I compute the estimated ATE manually\n",
    "    ATE_estimator_manual = np.mean(Y[:500]-Y[500:])\n",
    "    # I append the ATE_estimated manually to the respective array\n",
    "    ATE_estimator_manual0.append(ATE_estimator_manual)    \n",
    "\n",
    "    # I estimate the upper bound of the variance by extracting the heteroscedasticity robust standard errors and by squaring them to get the variance\n",
    "    # I use this approach as indicated in part 8, out of simplicity\n",
    "    V_ATE_plus = rlr_model.fit(cov='H1').bse[1]\n",
    "    V_ATE_plus = V_ATE_plus**2\n",
    "    # I append the upper bound of the variance to an array\n",
    "    Upper_bound_ATE_variance0.append(V_ATE_plus)\n",
    "\n",
    "    # Manual calculation of the upper bound of the variance of the ATE\n",
    "    Upper_bound_ATE_manual = (np.var(Y[:500],ddof= 1))/len(Y[:500])+(np.var(Y[500:],ddof= 1))/len(Y[500:])\n",
    "    # I append the manual estimate to the respective array\n",
    "    Upper_bound_ATE_manual0.append(Upper_bound_ATE_manual)    \n",
    "\n",
    "    # I defined the critical value of the confidence interval\n",
    "    critical_value = 1.96\n",
    "    # I define the lower bound of the confidence interval\n",
    "    lower_bound_IC = ATE_estimate_D-critical_value*np.sqrt(V_ATE_plus)\n",
    "    # I define the upper bound of the confidence interval\n",
    "    upper_bound_IC = ATE_estimate_D+critical_value*np.sqrt(V_ATE_plus)\n",
    "\n",
    "    # If statement: if the actual ATE (estimated in point 4) is in confidence interval, I append 1 to list\n",
    "    # Else, I append 0 to the list\n",
    "    if ATE_1000 <= upper_bound_IC and ATE_1000 >= lower_bound_IC:\n",
    "        Indicator_ATEparam_inCI0.append(1)\n",
    "    else:\n",
    "        Indicator_ATEparam_inCI0.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ATE estimate 0.2068719047658548\n",
      "Difference between average ATE estimate and actual ATE -0.003904528892679948\n",
      "Variance of ATE estimates 0.0024672024817506957\n",
      "Difference between ATE estimate variance and mean upper bound variance -0.0005394522661341544\n",
      "Mean Indicator CI 0.975\n"
     ]
    }
   ],
   "source": [
    "# I compute the mean of ATE estimates\n",
    "mean_ATE_estimate = np.mean(ATE_estimator_list0)\n",
    "\n",
    "print(\"Mean ATE estimate\", mean_ATE_estimate)\n",
    "print(\"Difference between average ATE estimate and actual ATE\", ATE_1000-mean_ATE_estimate)\n",
    "\n",
    "# I compute the variance of ATE estimates\n",
    "variance_ATE_estimate = np.var(ATE_estimator_list0)\n",
    "print(\"Variance of ATE estimates\", variance_ATE_estimate)\n",
    "print(\"Difference between ATE estimate variance and mean upper bound variance\", variance_ATE_estimate-np.mean(Upper_bound_ATE_variance0))\n",
    "\n",
    "# I compute the mean of the CI indicator function results\n",
    "average_indicator_CI = np.mean(Indicator_ATEparam_inCI0)\n",
    "print(\"Mean Indicator CI\", average_indicator_CI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ATE estimator manually calculated 0.2059609948225195\n",
      "Mean Upper Bound Var ATE Estimator manually calculated 0.0031154704866156622\n",
      "\n",
      "Mean ATE estimator calculated with rreg 0.2068719047658548\n",
      "Mean Upper Bound Var ATE Estimator calculated with rreg 0.00300665474788485\n",
      "\n",
      "The results are roughly the same\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean ATE estimator manually calculated\",np.mean(ATE_estimator_manual0))\n",
    "print(\"Mean Upper Bound Var ATE Estimator manually calculated\", np.mean(Upper_bound_ATE_manual0))\n",
    "\n",
    "print(\"\\nMean ATE estimator calculated with rreg\",np.mean(ATE_estimator_list0))\n",
    "print(\"Mean Upper Bound Var ATE Estimator calculated with rreg\", np.mean(Upper_bound_ATE_variance0))\n",
    "\n",
    "print(\"\\nThe results are roughly the same\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're working in a finite population perspective, and by the design of the loop the RAT Assumption is satisfied, then the ATE estimator is unbiased, and this is empirically confirmed by the fact that the mean ATE estimate is very close to the actual ATE.\n",
    "\n",
    "(Slide 77 - slides_RCT) - Theorem: $\\hat{ATE}_{n}$ is an unbiased estimator of $ATE_{n}$\n",
    "\n",
    "As stated in point 3, here the treatment effect is heterogeneous, and therefore the upper bound of the ATE variance is not an unbiased estimator of the variance, but it tends to overestimate the parameter. As a consequence, it makes sense that its average is larger than the standard deviation of the ATE estimates.\n",
    "\n",
    "(Slide 118 - slides_RCT)\n",
    "\n",
    "As a consequence of the above mentioned results, it makes sense that over 800 iterations, more than 95% of the times the actual ATE is within the confidence interval (since the IC we're using is larger than the one we'd get with the actual standard deviation of the ATE).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I assign to the variable iterations the number of times the loop will have to repeat itself\n",
    "iterations = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list below is where I will store the ATE estimates for each iteration\n",
    "ATE_estimator_list1 = []\n",
    "# The list below is where I will store the upper bound ATE variance estimates for each iteration\n",
    "Upper_bound_ATE_variance1 = []\n",
    "# The list below is where I will the outcome of the indicator function wrt to the confidence interval\n",
    "# As indicated in the instructions\n",
    "# The indicator that is 1 if the actual ATE is in the confidence interval and 0 otherwise\n",
    "Indicator_ATEparam_inCI1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I assign to the following variable the actual ATE amount for this exercise\n",
    "ATE_1000_p7 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I define the for loop (800 iterations, as stated above)\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    # I define the variable y_0 which takes 1000 normally distributed values (it's a vector) (standard normal distr)\n",
    "    y_0 = np.random.normal(size = 1000)\n",
    "    \n",
    "    # I define the variable V which takes 1000 normally distributed values (it's a vector)\n",
    "    # The way its values are determined is of course independent from what's going on with y_0\n",
    "    V = np.random.normal(size = 1000)\n",
    "\n",
    "    # I define the varialbe y_1 according to the rules indicated in the instructions\n",
    "    y_1 = 0.5*y_0+0.5*V+0.2\n",
    "\n",
    "    # I store y_0 and y_1 for each observation in a dataframe called units\n",
    "    units = pd.DataFrame({\"Y0\": y_0, \"Y1\": y_1})\n",
    "\n",
    "    # I create a vector of length 1000 with random numbers extracted from a uniform distribution\n",
    "    r_num = np.random.uniform(0, 1, 1000)\n",
    "\n",
    "    # I add to the dataframe units a column with the random numbers, and then I sort the dataframe\n",
    "    # based on this new column of random numbers\n",
    "    units.insert(2, \"Random Number\", r_num, False)\n",
    "    units = units.sort_values(by=\"Random Number\")\n",
    "\n",
    "    # I create a dummy variable D equal to 1 for the first 500 observations in the sorted dataset\n",
    "    D = [1]*500+[0]*500\n",
    "    \n",
    "    # I add this dummy variable as the 4th column of the dataframe\n",
    "    units.insert(3, \"Dummy\", D)\n",
    "\n",
    "    # I create a variable Y = (1-D)y(0)+Dy(1)\n",
    "    Y = (np.ones(1000)-units[\"Dummy\"].values)*units[\"Y0\"].values+units[\"Dummy\"].values*units[\"Y1\"].values\n",
    "    \n",
    "    # I manually compute the value of ATE_1000\n",
    "    ATE_estimator = np.mean(Y[:500]-Y[500:])\n",
    "    # I append the value to the respective array\n",
    "    ATE_estimator_list1.append(ATE_estimator)\n",
    "\n",
    "    # I manually compute the value of the upper bound of the variance of the ATE estimator\n",
    "    upper_bound_variance_estimate = (np.var(Y[:500],ddof= 1))/len(Y[:500])+(np.var(Y[500:],ddof= 1))/len(Y[500:])\n",
    "    # I append the estimate above to the respective array\n",
    "    Upper_bound_ATE_variance1.append(upper_bound_variance_estimate)\n",
    "\n",
    "    # I define the critical value for the confidence interval\n",
    "    critical_value = 1.96\n",
    "\n",
    "    # I compute the lower bound and the upper boiund for the confidence interval\n",
    "    lower_bound_IC = ATE_estimator-critical_value*np.sqrt(upper_bound_variance_estimate)\n",
    "    upper_bound_IC = ATE_estimator+critical_value*np.sqrt(upper_bound_variance_estimate)\n",
    "\n",
    "    # I define the indicator function which returns 1 if 0.2 is in the confidence interval and 0 otherwise\n",
    "    if ATE_1000_p7 <= upper_bound_IC and ATE_1000_p7 >= lower_bound_IC:\n",
    "        Indicator_ATEparam_inCI1.append(1)\n",
    "    else:\n",
    "        Indicator_ATEparam_inCI1.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ATE estimates 0.19887301511237152\n",
      "Difference between average ATE estimate and actual ATE 0.001126984887628496\n",
      "Variance of ATE estimates 0.0030577796077261767\n",
      "Difference between ATE estimate variance and mean upper bound variance 6.147597658202279e-05\n",
      "Mean Indicator CI 0.9525\n"
     ]
    }
   ],
   "source": [
    "# I calculate the mean of the ATE_estimator_list1 array\n",
    "mean_ATE_estimate = np.mean(ATE_estimator_list1)\n",
    "\n",
    "print(\"Mean ATE estimates\", mean_ATE_estimate)\n",
    "print(\"Difference between average ATE estimate and actual ATE\", 0.2-mean_ATE_estimate)\n",
    "\n",
    "# I calculate the variance of the ATE_estimator_list1 array\n",
    "variance_ATE_estimate = np.var(ATE_estimator_list1)\n",
    "print(\"Variance of ATE estimates\", variance_ATE_estimate)\n",
    "print(\"Difference between ATE estimate variance and mean upper bound variance\", variance_ATE_estimate-np.mean(Upper_bound_ATE_variance1))\n",
    "\n",
    "# I calculate the mean of the average_indicator_CI array, which contains the outcomes of the\n",
    "# indicator function \n",
    "average_indicator_CI = np.mean(Indicator_ATEparam_inCI1)\n",
    "print(\"Mean Indicator CI\", average_indicator_CI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, by constantly randomizing the potential outcomes in each iteration of the loop, we're simulating sampling from an infinite population. By the way the code is written, we're also fulfilling the CRAT assumption.\n",
    "\n",
    "Since, even in this case, the ATE estimator is unbiased, the fact that the mean of the estimations is very close to the actual ATE parameter is in line with expectations (not a pun).\n",
    "\n",
    "Reference: Theorem: $\\hat{ATE}_{n}$  is an unbiased estimator of $ATE$ (page 193 - slides_RCT)\n",
    "\n",
    "Furthermore, differently from the previous part (Part 7), under the assumptions we're using now we can reliably estimate the variance with an unbiased estimator (Theorem: the variance of $\\hat{ATE}_{n}$  page 200 - slides_RCT) and this justifies the effect that the mean of the variances that we've estimated is very close to the variance of the ATE estimates.\n",
    "\n",
    "In this case, the confidence interval is more precise than the one of the previous exercise, and therefore the fact that the actual ATE is inside the interval around 95% of the times is in line with the theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I assign to the variable iterations the number of times the loop will have to repeat itself\n",
    "iterations = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list below is where I will store the ATE estimates for each iteration\n",
    "ATE_estimator_list2 = []\n",
    "# The list below is where I will the outcome of the indicator function wrt to the confidence interval\n",
    "Upper_bound_ATE_variance2 = []\n",
    "# As indicated in the instructions\n",
    "# The indicator that is 1 if the actual ATE is in the confidence interval and 0 otherwise\n",
    "Indicator_ATEparam_inCI2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I define the for loop (800 iterations, as stated above)\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    # I define the variable y_0 which takes 1000 normally distributed values (it's a vector) (standard normal distr)\n",
    "    y_0 = np.random.normal(size = 1000)\n",
    "\n",
    "    # I define the varialbe y_1 according to the rules indicated in the instructions\n",
    "    y_1 = y_0+0.1771\n",
    "\n",
    "    # I store y_0 and y_1 for each observation in a dataframe called units\n",
    "    units = pd.DataFrame({\"Y0\": y_0, \"Y1\": y_1})\n",
    "    # I create a vector of length 1000 with random numbers extracted from a uniform distribution\n",
    "    r_num = np.random.uniform(0, 1, 1000)\n",
    "\n",
    "    # I add to the dataframe units a column with the random numbers, and then I sort the dataframe\n",
    "    # based on this new column of random numbers\n",
    "    units.insert(2, \"Random Number\", r_num, False)\n",
    "    units = units.sort_values(by=\"Random Number\")\n",
    "\n",
    "    # I create a dummy variable D equal to 1 for the first 500 observations in the sorted dataset\n",
    "    D = [1]*500+[0]*500\n",
    "    # I add this dummy variable as the 4th column of the dataframe  \n",
    "    units.insert(3, \"Dummy\", D)\n",
    "\n",
    "    # I create a variable Y = (1-D)y(0)+Dy(1)\n",
    "    Y = (np.ones(1000)-units[\"Dummy\"].values)*units[\"Y0\"].values+units[\"Dummy\"].values*units[\"Y1\"].values\n",
    "\n",
    "    # I regress Y on D using the robust option\n",
    "    rlr_model = sm.RLM(Y,sm.add_constant(units[\"Dummy\"].values), M=sm.robust.norms.HuberT())\n",
    "    # I estract the coefficient of the dummy variable, since it should correspond to the estimate of ATE\n",
    "    ATE_estimate_D = rlr_model.fit().params[1]\n",
    "    # I append to a Python list the number ATE_estimate_D\n",
    "    ATE_estimator_list2.append(ATE_estimate_D)\n",
    "\n",
    "    # I estimate the upper bound of the variance\n",
    "    V_ATE_plus = rlr_model.fit(cov='H1').bse[1]\n",
    "    V_ATE_plus = V_ATE_plus**2\n",
    "    # I append the upper bound of the variance to an array\n",
    "    Upper_bound_ATE_variance2.append(V_ATE_plus)\n",
    "\n",
    "    # I define the critical value for the confidence interval\n",
    "    critical_value = 1.96\n",
    "\n",
    "    # I define the t-value using the formula in step 7 of the instructions\n",
    "    t_value = np.abs(ATE_estimate_D/np.sqrt(V_ATE_plus))\n",
    "\n",
    "    # I simulate the indicator function of step 7 and store the result in an array\n",
    "    if t_value > critical_value:\n",
    "        Indicator_ATEparam_inCI2.append(1)\n",
    "    else:\n",
    "        Indicator_ATEparam_inCI2.append(0)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CI Indicator Result 0.78\n"
     ]
    }
   ],
   "source": [
    "# I compute the mean of the results of the indicator function and print it\n",
    "average_indicator_CI = np.mean(Indicator_ATEparam_inCI2)\n",
    "print(\"Mean CI Indicator Result\", average_indicator_CI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, according to the way we defined the variable `y_0` (a variable coming from the standard normal), its variance is 1, and since `y_1 = y_0+0.1771`, where 0.1771 is a constant, then it has the same variance as `y_0`, which is 1. \n",
    "\n",
    "Considering that we're simulating sampling from an infinite population, and especially that in this case the traatment effect is homogeneous (0.1771), $\\hat{V}(\\hat{ATE}_{n})=\\frac{V(Y(0))}{n-n_{1}}+\\frac{V(Y(1))}{n_{1}}$.\n",
    "\n",
    "So, with reference to the MDD calculation, $\\frac{0.1771}{\\sqrt{0.004}}\\approx 2.80=(q_{1-\\frac{\\alpha}{2}}+q_{\\lambda})$.\n",
    "\n",
    "Since we've using a critical value of 1.96, in the exercise we assumed a level of 5% in a double-tailed test, and therefore, under this conditions, the power of the test is: $$\\phi(2.80-q_{1-\\frac{0.05}{2}})=\\phi(q_{\\lambda})$$\n",
    "\n",
    "$$=\\phi(2.80-1.96)=0.80= \\lambda$$\n",
    "\n",
    "Hence, it makes sense that mean of the indicator functions results are around 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "Assume you want to use a randomized experiment to measure the effect of a treatment.\n",
    "\n",
    "Your experiment will have 1000 subjects. 500 will be treated, while 500 will remain untreated. Moreover, given the nature of the treatment, you think it makes sense to assume that $V (Y (0))$ = $V (Y (1))$. \n",
    "\n",
    "No one has ever measured the effect of the specific treatment you are interested in, but a literature review of the effects of treatments with a similar cost shows that they typically produce effects in the range of 10% of a standard deviation of the outcome.\n",
    "\n",
    "Should you embark in this experiment?\n",
    "\n",
    "Hint: revise the lecture notes on MDD (end of first\n",
    "chapter) to refresh your memory on how to proceed when the MDD is expressed as standard\n",
    "deviations of the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Working with the same data that we used for **Part 8** and under the same setting, we know that the an effect in the range of 10% of a standard deviation of the outcome would be below the MDD that we'd get with a power of 0.8 and an alpha of 0.05. Indeed, as we saw in the previous exercise, that result is 0.1771, which corresponds to 17.71%.\n",
    "\n",
    "Therefore, we wouldn't be able to estimate that effect with enough power, so it would probably not be advisable to embark in this experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power 0.47459866124544475\n",
      "The power we get in this case is quite low, and therefore it's better not to embark in the experiemnt\n"
     ]
    }
   ],
   "source": [
    "# I assign to a variable the value of the expected effect\n",
    "effect = 0.1\n",
    "# I assign to a variable the value of the variance determined in part 8\n",
    "variance = 0.004\n",
    "\n",
    "# I calculate the ratio between effect and standard deviations to get\n",
    "# the sum of the two critical values, for the alpha and the power\n",
    "total_q = effect/np.sqrt(variance)\n",
    "\n",
    "# I assign the alpha to a variable\n",
    "alpha = 0.05\n",
    "\n",
    "# I subtract to the sum of the two critical values the critical value of the alpha to extract the \n",
    "# one of the power\n",
    "power_q = total_q- stats.norm.ppf(1-alpha)\n",
    "# I extract the power in percentage with the CDF of the standard normal\n",
    "power_percent = stats.norm.cdf(power_q)\n",
    "print(\"Power\", power_percent)\n",
    "print(\"The power we get in this case is quite low, and therefore it's better not to embark in the experiemnt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I assign to the variable iterations the number of times the loop will have to repeat itself\n",
    "iterations = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this array I will store the outcomes of the treatment effect estimates\n",
    "ATE_estimator_list3 = []\n",
    "# In this list I will store the outcomes of the indicator function that checks the outcome\n",
    "# of the randomization inference test\n",
    "Indicator_ATEparam_inCI3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I define the for loop (800 iterations, as stated above)\n",
    "\n",
    "for i in range(iterations):\n",
    "    # I define the variable y_0 which takes 10 normally distributed values (it's a vector) (standard normal distr)\n",
    "    y_0 = np.random.normal(size = 10)\n",
    "    # I define y_1 as equal to y_0 since we're simulating an experiment where the sharp null is \n",
    "    # actually true\n",
    "    y_1 = y_0\n",
    "\n",
    "    # I store y_0 and y_1 for each observation in a dataframe called units\n",
    "    units = pd.DataFrame({\"Y0\": y_0, \"Y1\": y_1})\n",
    "    # I create a vector of length 10 with random numbers extracted from a uniform distribution\n",
    "    r_num = np.random.uniform(0, 1, 10)\n",
    "\n",
    "    # I add to the dataframe units a column with the random numbers, and then I sort the dataframe\n",
    "    # based on this new column of random numbers\n",
    "    units.insert(2, \"Random Number\", r_num, False)\n",
    "    units = units.sort_values(by=\"Random Number\")\n",
    "\n",
    "    # I create a dummy variable D equal to 1 for the first 5 observations in the sorted dataset\n",
    "    D = [1]*5+[0]*5\n",
    "    # I add this dummy variable as the 4th column of the dataframe  \n",
    "    units.insert(3, \"Dummy\", D)\n",
    "    # I create a variable Y = (1-D)y(0)+Dy(1)\n",
    "    Y = (np.ones(10)-units[\"Dummy\"].values)*units[\"Y0\"].values+units[\"Dummy\"].values*units[\"Y1\"].values\n",
    "    # I regress Y on D using the OLS\n",
    "    reg = sm.OLS(Y, sm.add_constant(D)).fit()\n",
    "    # I estract the coefficient of the dummy variable, since it should correspond to the estimate of ATE\n",
    "    ATE_1000_estimate = reg.params[1]\n",
    "    # I append to a Python list the number ATE_estimate_D\n",
    "    ATE_estimator_list3.append(ATE_1000_estimate)\n",
    "    # I define the array where I'll store the coefficients of the randomization inference test\n",
    "    Reg_coefficients = []\n",
    "\n",
    "    # I start the for loop with 100 repetitions\n",
    "    for j in range(100):\n",
    "        # I create a variable that's a copy of y_0\n",
    "        y_0_copy = y_0.copy()\n",
    "        # I create a variable that's a copy of y_1\n",
    "        y_1_copy = y_1.copy()\n",
    "        # I store y_0 and y_1 for each observation in a dataframe called units\n",
    "        units1 = pd.DataFrame({\"Y0\": y_0_copy, \"Y1\": y_1_copy})\n",
    "        # I create a vector of length 10 with random numbers extracted from a uniform distribution\n",
    "        r_num1 = np.random.uniform(0, 1, 10)\n",
    "\n",
    "        # I add to the dataframe units1 a column with the random numbers, and then I sort the dataframe\n",
    "        # based on this new column of random numbers\n",
    "        units1.insert(2, \"Random Number\", r_num1, False)\n",
    "        units1 = units1.sort_values(by=\"Random Number\")\n",
    "\n",
    "        # I create a dummy variable D equal to 1 for the first 5 observations in the sorted dataset\n",
    "        D_tilde = [1]*5+[0]*5\n",
    "        # I add this dummy variable as the 4th column of the dataframe\n",
    "        units1.insert(3, \"Dummy\", D_tilde)\n",
    "        # I create a variable Y = (1-D)y(0)+Dy(1)\n",
    "        Y = (np.ones(10)-units1[\"Dummy\"].values)*units1[\"Y0\"].values+units1[\"Dummy\"].values*units[\"Y1\"].values\n",
    "\n",
    "        # I regress Y on D using the OLS (note that I awlays add a constant when I regress, as you can see)\n",
    "        reg1 = sm.OLS(Y,sm.add_constant(D_tilde)).fit()\n",
    "        # I append the coefficient of the dummy to the matrix Reg_coefficients that I've defined above\n",
    "        Reg_coefficients.append(reg1.params[1])\n",
    "\n",
    "    # I extract the 5th percentile value amount the coefficients I found in the randomization above \n",
    "    Percentile_5_regcoeff = np.percentile(Reg_coefficients,5)\n",
    "    # I extract the 95th percentile value amount the coefficients I found in the randomization above\n",
    "    Percentile_95_regcoeff = np.percentile(Reg_coefficients,95)\n",
    "\n",
    "    # I simulate the indicator function. If my first ATE estimate is between the 5th and 95th percentile\n",
    "    # of the coefficients introduced during the randomization I return 0, else 1\n",
    "    if ATE_1000_estimate >= Percentile_5_regcoeff and ATE_1000_estimate <= Percentile_95_regcoeff:\n",
    "        Indicator_ATEparam_inCI3.append(0)\n",
    "    else:\n",
    "        Indicator_ATEparam_inCI3.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09875\n"
     ]
    }
   ],
   "source": [
    "# I compute the mean of the results of the indicator function and print it\n",
    "print(np.mean(Indicator_ATEparam_inCI3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we're simulating several randomization inference tests in a situation in which, by design, the sharp null we're testing against is actually true.\n",
    "\n",
    "In this exercise we're using for each randomization inference test a confidence interval with a level of 10%, and therefore, it's coherent with the theory the fact that over all the 800 iterations we've only rejected the null around 10% of the times.\n",
    "\n",
    "This works even if we only worked with 10 observations, because, as indicated in slide 142, randomization inference is fit to the case in which the sample size is not going to infinity, so in the non asymptotic case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ATE \\equiv E(Y(1)-Y(0))$$\n",
    "$$= E(Y(1))-E(Y(0)) \\text{ (by linearity of expectation)}$$\n",
    "\n",
    "Since\n",
    "$$E(Y(1)) = E(Y(1)|D=1)P(D=1)+E(Y(1)|D=0)P(D=0)$$\n",
    "$$E(Y(0)) = E(Y(0)|D=1)P(D=1)+E(Y(0)|D=0)P(D=0)$$\n",
    "\n",
    "Then\n",
    "$$ATE \\equiv E(Y(1)-Y(0)) =  E(Y(1))-E(Y(0))$$\n",
    "$$=E(Y(1)|D=1)P(D=1)+E(Y(1)|D=0)P(D=0)-E(Y(0)|D=1)P(D=1)-E(Y(0)|D=0)P(D=0)$$\n",
    "\n",
    "**Terms that are observable**\n",
    "- $E(Y(1)|D=1)P(D=1)$ (So both $E(Y(1)|D=1)$ and $P(D=1)$)\n",
    "- $E(Y(0)|D=0)P(D=0)$ (So both $E(Y(0)|D=0)$ and $P(D=0)$)\n",
    "\n",
    "**Terms that are NOT observable**\n",
    "- $E(Y(0)|D=1)$\n",
    "- $E(Y(1)|D=0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By **Assumption 1**, we know that $Y(0) \\in [l,h], Y(1) \\in [l,h]$.\n",
    "\n",
    "Hence, we can define a lower and upper bound for the ATE by bounding the unobservable terms.\n",
    "\n",
    "**Upper Bound:**\n",
    "$$E(Y(1)|D=1)P(D=1) + hP(D=0) - lP(D=1) - E(Y(0)|D=0)P(D=0)$$\n",
    "\n",
    "**Lower Bound:**\n",
    "$$E(Y(1)|D=1)P(D=1) + lP(D=0) - hP(D=1) - E(Y(0)|D=0)P(D=0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y(0),Y(1) \\in [l,h]$, then $E[Y(0)],E[Y(1)] \\in [l,h]$.\n",
    "\n",
    "First of all, notice that $l-h \\leq 0 \\leq h-l$.\n",
    "\n",
    "Then, \n",
    "$$l-h \\leq Y(1)-Y(0) \\leq h-l$$\n",
    "$$l-h \\leq E[Y(1)-Y(0)] \\leq h-l$$\n",
    "$$l-h \\leq ATE \\leq h-l$$\n",
    "\n",
    "Indeed, the smallest value that the **lower bound** can attain is:\n",
    "$$lP(D=1)+lP(D=0)-hP(D=1)-hP(D=0)=(l-h)(P(D=1)+P(D=0))=l-h$$\n",
    "\n",
    "We attain this by setting:\n",
    "- $E(Y(1)|D=0)=l$\n",
    "- $E(Y(0)|D=1)=h$\n",
    "\n",
    "\n",
    "While the greatest value that it may attain is:\n",
    "$$lP(D=1)-lP(D=1)+hP(D=0)-hP(D=0)=0$$\n",
    "\n",
    "\n",
    "We attain this by setting:\n",
    "- $E(Y(1)|D=0)=h$\n",
    "- $E(Y(0)|D=1)=l$\n",
    "\n",
    "\n",
    "In the case of the **upper bound**, on the contrary, we have as smallest value:\n",
    "$$hP(D=1)-lP(D=0)+lP(D=0)-hP(D=1)=0$$\n",
    "\n",
    "We attain this by setting:\n",
    "- $E(Y(1)|D=0)=l$\n",
    "- $E(Y(0)|D=1)=h$\n",
    "\n",
    "And we have as largest attainable value:\n",
    "$$hP(D=1)-lP(D=0)+hP(D=0)-lP(D=1)=(h-l)(P(D=1)+P(D=0))=h-l$$\n",
    "\n",
    "We attain this by setting:\n",
    "- $E(Y(1)|D=0)=h$\n",
    "- $E(Y(0)|D=1)=l$\n",
    "\n",
    "So, in general, the bounds constructed in question 2 will always, in any case, contain 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
